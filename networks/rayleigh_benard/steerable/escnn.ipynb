{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe3a3592730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from escnn.gspaces import GSpace\n",
    "from escnn.group import Representation\n",
    "from typing import Literal, Callable, Any\n",
    "from escnn.nn import FieldType, GeometricTensor\n",
    "from escnn import nn as escnn_nn\n",
    "\n",
    "\n",
    "class RBSteerableConv(escnn_nn.EquivariantModule):\n",
    "    def __init__(self, \n",
    "                 gspace: GSpace, \n",
    "                 in_fields: tuple[Representation] | list[Representation], \n",
    "                 out_fields: tuple[Representation] | list[Representation], \n",
    "                 in_dims: int,\n",
    "                 v_kernel_size: int,\n",
    "                 h_kernel_size: int,\n",
    "                 v_stride: int = 1,\n",
    "                 h_stride: int = 1,\n",
    "                 h_dilation: int = 1,\n",
    "                 v_pad_mode: Literal['valid', 'zero'] = 'zero', \n",
    "                 h_pad_mode: Literal['valid', 'zero', 'circular', 'reflect', 'replicate'] = 'circular',\n",
    "                 bias: bool = True,\n",
    "                 sigma: float | list[float] = None,\n",
    "                 frequencies_cutoff: float | Callable[[float], int] = None,\n",
    "                 rings: list[float] = None,\n",
    "                 maximum_offset: int = None,\n",
    "                 recompute: bool = False,\n",
    "                 basis_filter: Callable[[dict], bool] = None,\n",
    "                 initialize: bool = True,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(in_dims) == 3\n",
    "        \n",
    "        v_pad_mode = v_pad_mode.lower()\n",
    "        h_pad_mode = h_pad_mode.lower()\n",
    "        assert v_pad_mode.lower() in ['valid', 'zero']\n",
    "        assert h_pad_mode.lower() in ['valid', 'zero', 'circular', 'reflect', 'replicate']\n",
    "        \n",
    "        if h_pad_mode == 'valid':\n",
    "            h_padding = 0\n",
    "            h_pad_mode = 'zero'\n",
    "        else:\n",
    "            # escnn_nn.R2Conv only allows for the same amount of padding on both sides\n",
    "            h_padding = [compute_required_same_padding(in_dims[i], h_kernel_size, h_stride, split=True)[1] for i in [0, 1]]\n",
    "        \n",
    "        out_height = compute_output_size(in_dims[-1], v_kernel_size, v_stride, dilation=1, pad=v_pad_mode!='valid')\n",
    "        \n",
    "        r2_conv_in_type = FieldType(gspace, out_height*v_kernel_size*in_fields) # concatenated neighborhoods\n",
    "        out_type = FieldType(gspace, out_height*out_fields)\n",
    "\n",
    "        self.r2_conv = escnn_nn.R2Conv(in_type=r2_conv_in_type, \n",
    "                                       out_type=out_type, \n",
    "                                       kernel_size=h_kernel_size, \n",
    "                                       padding=tuple(h_padding), \n",
    "                                       stride=h_stride, \n",
    "                                       dilation=h_dilation,\n",
    "                                       padding_mode=h_pad_mode,\n",
    "                                       groups=out_height, \n",
    "                                       bias=bias,\n",
    "                                       sigma=sigma,\n",
    "                                       frequencies_cutoff=frequencies_cutoff,\n",
    "                                       rings=rings,\n",
    "                                       maximum_offset=maximum_offset,\n",
    "                                       recompute=recompute,\n",
    "                                       basis_filter=basis_filter,\n",
    "                                       initialize=initialize,\n",
    "                                       **kwargs)\n",
    "        \n",
    "        self.in_fields = in_fields\n",
    "        self.out_fields = out_fields\n",
    "        \n",
    "        self.in_height = in_dims[-1]\n",
    "        self.out_height = out_height\n",
    "        \n",
    "        self.in_type = FieldType(gspace, self.in_height*self.in_fields) # without any neighborhood concatenation\n",
    "        self.r2_conv_in_type = r2_conv_in_type # with any neighborhood concatenation\n",
    "        self.out_type = out_type\n",
    "        \n",
    "        self.in_dims = in_dims\n",
    "        # escnn_nn.R2Conv only allows for the same amount of padding on both sides\n",
    "        self.out_dims = [compute_output_size(in_dims[i], h_kernel_size, h_stride, dilation=h_dilation, \n",
    "                                             pad=h_pad_mode!='valid', equal_pad=True) for i in [0, 1]] + [out_height]\n",
    "        \n",
    "        self.v_pad = v_pad_mode!='valid'\n",
    "        self.v_stride = v_stride\n",
    "        self.v_kernel_size = v_kernel_size\n",
    "        \n",
    "        \n",
    "    def forward(self, input: GeometricTensor) -> GeometricTensor:\n",
    "        \"\"\"\n",
    "        geomTensor of shape [batch, inHeight*sum(inFieldsizes), width, depth]\n",
    "        -> geomTensor of shape [batch, outHeight*sum(outFieldsizes), width, depth]\n",
    "        \"\"\"\n",
    "        assert input.type == self.in_type\n",
    "        \n",
    "        concatenated_neighborhoods = self._concat_vertical_neighborhoods(input)\n",
    "        return self.r2_conv.forward(concatenated_neighborhoods)\n",
    "        \n",
    "        \n",
    "    def _concat_vertical_neighborhoods(self, geom_tensor: GeometricTensor) -> GeometricTensor:\n",
    "        \"\"\"geomTensor of shape [batch, inHeight*sum(fieldsizes), width, depth]\n",
    "        -> [batch, outHeight*ksize*sum(fieldsizes), width, depth]\"\"\"\n",
    "        tensor = geom_tensor.tensor.reshape(-1, self.in_height, sum(field.size for field in self.in_fields), *self.in_dims[:2])\n",
    "\n",
    "        if self.v_pad:\n",
    "            # pad height\n",
    "            padding = compute_required_same_padding(in_size=self.in_height, kernel_size=self.v_kernel_size, stride=self.v_stride, split=True)\n",
    "            tensor = F.pad(tensor, (*([0,0]*3), *padding)) # shape:(b,padH,c*t,w,d)\n",
    "        \n",
    "        # compute neighborhoods\n",
    "        tensor = tensor.unfold(dimension=1, size=self.v_kernel_size, step=self.v_stride) # shape:(b,outH,c*t,w,d,ksize)\n",
    "        \n",
    "        # concatenate neighboroods\n",
    "        tensor = tensor.permute(0, 1, 5, 2, 3, 4) # shape:(b,outH,ksize,c*t,w,d)\n",
    "        tensor = tensor.flatten(start_dim=1, end_dim=3) # shape:(b,outH*ksize*c*t,w,d)\n",
    "        \n",
    "        return GeometricTensor(tensor, self.r2_conv_in_type)\n",
    "    \n",
    "    \n",
    "    def evaluate_output_shape(self, input_shape: tuple) -> tuple:\n",
    "        assert len(input_shape) == 4\n",
    "        assert input_shape[1] == self.in_type.size\n",
    "    \n",
    "        batch_size = input_shape[0]\n",
    "        \n",
    "        return (batch_size, self.out_type.size) + tuple(self.in_dims[:2])\n",
    "    \n",
    "    \n",
    "    def train(self, *args, **kwargs):\n",
    "        return self.r2_conv.train(*args, **kwargs)\n",
    "    \n",
    "    \n",
    "    def check_equivariance(self, atol: float = 1e-7, rtol: float = 1e-5) -> list[tuple[Any, float]]:\n",
    "        r\"\"\"\n",
    "        \n",
    "        Method that automatically tests the equivariance of the current module.\n",
    "        The default implementation of this method relies on :meth:`escnn.nn.GeometricTensor.transform` and uses the\n",
    "        the group elements in :attr:`~escnn.nn.FieldType.testing_elements`.\n",
    "        \n",
    "        This method can be overwritten for custom tests.\n",
    "        \n",
    "        Returns:\n",
    "            a list containing containing for each testing element a pair with that element and the corresponding\n",
    "            equivariance error\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        x = torch.randn(3, self.in_type.size, *self.in_dims[:2])\n",
    "        x = GeometricTensor(x, self.in_type)\n",
    "        \n",
    "        errors = []\n",
    "        for el in self.out_type.testing_elements:\n",
    "            el = self.in_type.gspace.fibergroup.sample()\n",
    "            print(el)\n",
    "            \n",
    "            out1 = self(x).transform(el).tensor.detach().numpy()\n",
    "            out2 = self(x.transform(el)).tensor.detach().numpy()\n",
    "        \n",
    "            errs = out1 - out2\n",
    "            errs = np.abs(errs).reshape(-1)\n",
    "            print(el, errs.max(), errs.mean(), errs.var())\n",
    "        \n",
    "            assert np.allclose(out1, out2, atol=atol, rtol=rtol), \\\n",
    "                f'The error found during equivariance check with element \"{el}\" \\\n",
    "                    is too high: max = {errs.max()}, mean = {errs.mean()} var ={errs.var()}'\n",
    "            \n",
    "            errors.append((el, errs.mean()))\n",
    "        \n",
    "        return errors\n",
    "\n",
    "        \n",
    "def compute_output_size(in_size: int, kernel_size: int, stride: int, dilation: int, pad: bool, equal_pad: bool = False) -> int:\n",
    "    padding = 0\n",
    "    if pad:\n",
    "        pad_split = compute_required_same_padding(in_size, kernel_size, stride, split=True)\n",
    "        padding = 2*pad_split[1] if equal_pad else sum(pad_split)\n",
    "\n",
    "    return ((in_size - dilation*(kernel_size-1) + padding - 1) // stride) + 1\n",
    "\n",
    "\n",
    "def compute_required_same_padding(in_size: int, kernel_size: int, stride: int, split: bool = False) -> int | tuple[int, int]:\n",
    "    out_size = math.ceil(in_size/stride)\n",
    "    padding = max((out_size-1) * stride - in_size + kernel_size, 0)\n",
    "    \n",
    "    if split:\n",
    "        return math.floor(padding/2), math.ceil(padding/2)\n",
    "    else:\n",
    "        return padding\n",
    "        \n",
    "        \n",
    "# TODO DataAugmentation (with vector rotation)\n",
    "# TODO 3D Pooling\n",
    "# TODO 3D Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "WIDTH, DEPTH, HEIGHT = 48, 48, 32\n",
    "RB_CHANNELS = 4\n",
    "\n",
    "sim_data = torch.randn(BATCH_SIZE, WIDTH, DEPTH, HEIGHT, RB_CHANNELS)\n",
    "\n",
    "tensor = sim_data.permute(0, 3, 4, 1, 2).reshape(BATCH_SIZE, HEIGHT*RB_CHANNELS, WIDTH, DEPTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from escnn import gspaces\n",
    "from collections import OrderedDict\n",
    "\n",
    "class RBModel(escnn_nn.SequentialModule):\n",
    "    def __init__(self, gspace: GSpace = gspaces.flipRot2dOnR2(N=4),\n",
    "                   rb_dims: tuple = (48, 48, 32),\n",
    "                   v_kernel_size: int = 3,\n",
    "                   h_kernel_size: int = 5,\n",
    "                   hidden_channels: tuple = 2*(10,10)\n",
    "                   ):      \n",
    "    \n",
    "        rb_fields = [gspace.trivial_repr, gspace.irrep(1, 1), gspace.trivial_repr]\n",
    "        hidden_field_type = [gspace.regular_repr]\n",
    "        \n",
    "        layers = OrderedDict()\n",
    "        \n",
    "        # INPUT LAYER\n",
    "        conv = RBSteerableConv(gspace=gspace, \n",
    "                            in_fields=rb_fields, \n",
    "                            out_fields=hidden_channels[0]*hidden_field_type, \n",
    "                            in_dims=rb_dims,\n",
    "                            v_kernel_size=v_kernel_size, \n",
    "                            h_kernel_size=h_kernel_size)\n",
    "        layers['Conv1'] = conv\n",
    "        \n",
    "        # HIDDEN LAYERS\n",
    "        for i, channels in enumerate(hidden_channels[1:], start=2):\n",
    "            conv = RBSteerableConv(gspace=gspace, \n",
    "                                in_fields=layers[f'Conv{i-1}'].out_fields, \n",
    "                                out_fields=channels*hidden_field_type, \n",
    "                                in_dims=layers[f'Conv{i-1}'].out_dims,\n",
    "                                v_kernel_size=v_kernel_size, \n",
    "                                h_kernel_size=h_kernel_size)\n",
    "            layers[f'Conv{i}'] = conv\n",
    "        \n",
    "        # OUTPUT LAYER\n",
    "        i += 1\n",
    "        conv = RBSteerableConv(gspace=gspace, \n",
    "                            in_fields=layers[f'Conv{i-1}'].out_fields, \n",
    "                            out_fields=rb_fields,\n",
    "                            in_dims=layers[f'Conv{i-1}'].out_dims,\n",
    "                            v_kernel_size=v_kernel_size, \n",
    "                            h_kernel_size=h_kernel_size)\n",
    "        layers[f'Conv{i}'] = conv\n",
    "        \n",
    "        super().__init__(layers)\n",
    "        \n",
    "    @property\n",
    "    def first_layer(self):\n",
    "        if len(self._modules) == 0:\n",
    "            return None\n",
    "        return self._modules[next(iter(self._modules))]\n",
    "        \n",
    "        \n",
    "    def check_equivariance(self, atol: float = 1e-4, rtol: float = 1e-5) -> list[tuple[Any, float]]:\n",
    "        r\"\"\"\n",
    "        \n",
    "        Method that automatically tests the equivariance of the current module.\n",
    "        The default implementation of this method relies on :meth:`escnn.nn.GeometricTensor.transform` and uses the\n",
    "        the group elements in :attr:`~escnn.nn.FieldType.testing_elements`.\n",
    "        \n",
    "        This method can be overwritten for custom tests.\n",
    "        \n",
    "        Returns:\n",
    "            a list containing containing for each testing element a pair with that element and the corresponding\n",
    "            equivariance error\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        x = torch.randn(3, self.in_type.size, *self.first_layer.in_dims[:2])\n",
    "        x = GeometricTensor(x, self.in_type)\n",
    "        \n",
    "        errors = []\n",
    "        for el in self.out_type.testing_elements:\n",
    "            el = self.in_type.gspace.fibergroup.sample()\n",
    "            print(el)\n",
    "            \n",
    "            out1 = self(x).transform(el).tensor.detach().numpy()\n",
    "            out2 = self(x.transform(el)).tensor.detach().numpy()\n",
    "        \n",
    "            errs = out1 - out2\n",
    "            errs = np.abs(errs).reshape(-1)\n",
    "            print(el, errs.max(), errs.mean(), errs.var())\n",
    "        \n",
    "            assert np.allclose(out1, out2, atol=atol, rtol=rtol), \\\n",
    "                f'The error found during equivariance check with element \"{el}\" \\\n",
    "                    is too high: max = {errs.max()}, mean = {errs.mean()} var ={errs.var()}'\n",
    "            \n",
    "            errors.append((el, errs.mean()))\n",
    "        \n",
    "        return errors\n",
    "\n",
    "model = RBModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-, 2[2pi/4])\n",
      "(-, 2[2pi/4]) 1.5258789e-05 2.061465e-06 2.5653188e-12\n",
      "(+, 3[2pi/4])\n",
      "(+, 3[2pi/4]) 1.5258789e-05 1.983555e-06 2.3780927e-12\n",
      "(+, 2[2pi/4])\n",
      "(+, 2[2pi/4]) 1.50203705e-05 2.1090534e-06 2.681103e-12\n",
      "(-, 1[2pi/4])\n",
      "(-, 1[2pi/4]) 1.335144e-05 1.8674493e-06 2.116885e-12\n",
      "(-, 3[2pi/4])\n",
      "(-, 3[2pi/4]) 1.4305115e-05 2.042686e-06 2.52035e-12\n",
      "(-, 1[2pi/4])\n",
      "(-, 1[2pi/4]) 1.335144e-05 1.8674493e-06 2.116885e-12\n",
      "(-, 2[2pi/4])\n",
      "(-, 2[2pi/4]) 1.5258789e-05 2.061465e-06 2.5653188e-12\n",
      "(+, 2[2pi/4])\n",
      "(+, 2[2pi/4]) 1.50203705e-05 2.1090534e-06 2.681103e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((-, 2[2pi/4]), 2.061465e-06),\n",
       " ((+, 3[2pi/4]), 1.983555e-06),\n",
       " ((+, 2[2pi/4]), 2.1090534e-06),\n",
       " ((-, 1[2pi/4]), 1.8674493e-06),\n",
       " ((-, 3[2pi/4]), 2.042686e-06),\n",
       " ((-, 1[2pi/4]), 1.8674493e-06),\n",
       " ((-, 2[2pi/4]), 2.061465e-06),\n",
       " ((+, 2[2pi/4]), 2.1090534e-06)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(model.in_type(tensor))\n",
    "model.check_equivariance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "escnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
